<p align="center">
  <h3 align="center"><strong>DeepMesh: Auto-Regressive Artist-Mesh Creation<br>With Reinforcement Learning</strong></h3>

<p align="center">
    <a href="https://zhaorw02.github.io/">Ruowen Zhao</a><sup>1,2*</sup>,
    <a href="https://jamesyjl.github.io/">Junliang Ye</a><sup>1,2*</sup>,
    <a href="https://thuwzy.github.io/">Zhengyi Wang</a><sup>1,2*</sup>,<br>
    <a href="">Guangce Liu</a><sup>2</sup>,
    <a href="https://buaacyw.github.io/">Yiwen Chen</a><sup>3</sup>,
    <a href="https://yikaiw.github.io/">Yikai Wang</a><sup>1</sup>,
    <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml">Jun Zhu</a><sup>1,2â€ </sup>
    <br>
    <sup>*</sup>Equal Contribution.
    <br>
    <sup>â€ </sup>Corresponding authors.
    <br>
    <sup>1</sup>Tsinghua University,
    <sup>2</sup>ShengShu,
    <br>
    <sup>3</sup>S-Lab, Nanyang Technological University,
</p>


<div align="center">

<a href='https://arxiv.org/abs/2503.15265'><img src='https://img.shields.io/badge/arXiv-2503.15265-b31b1b.svg'></a> &nbsp;&nbsp;&nbsp;&nbsp;
 <a href='https://zhaorw02.github.io/DeepMesh/'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp;&nbsp;&nbsp;&nbsp;
 <a><img src='https://img.shields.io/badge/License-MIT-blue'></a> &nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://huggingface.co/zzzrw/DeepMesh/tree/main"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Weights-HF-orange"></a> &nbsp;&nbsp;&nbsp;&nbsp;
<a href='https://www.youtube.com/watch?v=6grL7bSbQ2w'><img src='https://img.shields.io/badge/Youtube-Video-b31b1b.svg'>

</div>


<div align="center">

<img src="assets/teaser.png" alt="Demo" width="1024px" />

</div>

**All of the meshes above are generated by DeepMesh.** DeepMesh can generate high-quality meshes conditioned on the given point cloud by auto-regressive transformer.


## Release
- [3/20] ðŸ”¥ðŸ”¥We released the pretrained weight of **DeepMesh** (0.5 B).
- [4/01] ðŸ”¥We optimized the inference code, achieving a 50% reduction in generation time.

## Contents
- [Release](#release)
- [Installation](#installation)
- [Usage](#usage)
- [Important Notes](#important-notes)
- [Todo](#todo)
- [Acknowledgement](#acknowledgement)
- [BibTeX](#bibtex)

## Installation

#### 0. Prepare environment

Create docker or singularity image to use the Ubuntu 22, CUDA 11.8 with H100, and use this environment to install the libraries.

#### 1. Clone our repo and create conda environment
```
git clone https://github.com/zhaorw02/DeepMesh.git && cd DeepMesh
conda env create -f environment_slim.yaml
conda activate deepmesh
```

#### 2. Build flash-attention other modules,

```
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention
git checkout v2.7.0.post2
```

Comment out some lines in `flash-attention/csrc/{rotary,layer_norm,xentropy}/setup.py` to support your own GPU architecture to speed up the build like (e.g., H100):

```python
    if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None and CUDA_HOME is not None:
        _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)
        if bare_metal_version >= Version("11.8"):
            # os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0;8.6;9.0"
            os.environ["TORCH_CUDA_ARCH_LIST"] = "9.0"
        elif bare_metal_version >= Version("11.1"):
            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0;8.6"
        elif bare_metal_version == Version("11.0"):
            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0"
        else:
            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5"

# cc_flag.append("-gencode")
# cc_flag.append("arch=compute_70,code=sm_70")
# cc_flag.append("-gencode")
# cc_flag.append("arch=compute_80,code=sm_80")
if bare_metal_version >= Version("11.8"):
    cc_flag.append("-gencode")
    cc_flag.append("arch=compute_90,code=sm_90")
```

then,

```
cd csrc
cd rotary && time pip install .
cd ../layer_norm && time pip install .
cd ../xentropy && time pip install .
cd ..
```


#### 3. Install the pretrained model weight
```
pip install -U "huggingface_hub[cli]"
huggingface-cli login
huggingface-cli download zzzrw/DeepMesh --local-dir ./
```

## Usage
### Command line inference
```
# Note: if you want to use your own point cloud, please make sure the normal is included.

# Generate all obj/ply in your folder
CUDA_VISIBLE_DEVICES=0 torchrun --nproc-per-node=1 --master_port=12345 sample.py \
    --model_path "your_model_path" \
    --steps 90000 \
    --input_path examples \
    --output_path mesh_output \
    --repeat_num 4 \
    --temperature 0.5 \

# Generate the specified obj/ply in your folder
CUDA_VISIBLE_DEVICES=0 torchrun --nproc-per-node=1 --master_port=22345.py \
    --model_path "your_model_path" \
    --steps 90000 \
    --input_path examples \
    --output_path mesh_output \
    --repeat_num 4 \
    --uid_list "wand1.obj,wand2.obj,wand3.ply" \
    --temperature 0.5 \

# Or
bash sample.sh
```
## Important Notes
- Please refer to our [project_page](https://zhaorw02.github.io/DeepMesh/) for more examples.
## Todo
- [ ] Release of pre-training code  ( truncted sliding training ).
- [ ] Release of post-training code ( DPO ).
- [ ] Release of larger model ( 1b version ).

## Acknowledgement
Our code is based on these wonderful repos:
* **[BPT](https://github.com/Tencent-Hunyuan/bpt)**
* **[SMDM](https://github.com/ML-GSAI/SMDM)**
* [LLaMA-Mesh](https://github.com/nv-tlabs/LLaMa-Mesh)
* [Meshanything V2](https://github.com/buaacyw/MeshAnythingV2/tree/main)
* [Michelangelo](https://github.com/NeuralCarver/Michelangelo)

Also we invite you to explore our latest work [ShapeLLM-Omni](https://github.com/JAMESYJL/ShapeLLM-Omni) â€”â€” a native multimodal LLM for 3D generation and understanding.
## BibTeX
```
@article{zhao2025deepmesh,
  title={DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning},
  author={Zhao, Ruowen and Ye, Junliang and Wang, Zhengyi and Liu, Guangce and Chen, Yiwen and Wang, Yikai and Zhu, Jun},
  journal={arXiv preprint arXiv:2503.15265},
  year={2025}
}
```
